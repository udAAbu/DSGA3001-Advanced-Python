{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6  Part 2\n",
    "## CUDA programming with Numba\n",
    "### March 8, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "See [https://nyu-cds.github.io/python-numba/05-cuda/](https://nyu-cds.github.io/python-numba/05-cuda/).\n",
    "\n",
    "---\n",
    "If you graphics card support CUDA (Compute Unified Device Architecture) you must have a [CUDA driver](https://developer.nvidia.com/cuda-downloads) installed on your machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "Several important terms in the topic of CUDA programming are:\n",
    "\n",
    "**host**<br>\n",
    "&nbsp;&nbsp;&nbsp;the CPU<br>\n",
    "**host memory**<br>\n",
    "&nbsp;&nbsp;&nbsp;the system main memory<br>\n",
    "**device**<br>\n",
    "&nbsp;&nbsp;&nbsp;the GPU<br>\n",
    "**device memory**<br>\n",
    "&nbsp;&nbsp;&nbsp;onboard memory on a GPU card<br>\n",
    "**kernel**<br>\n",
    "&nbsp;&nbsp;&nbsp;a GPU function launched by the host (CPU) and executed on the device (GPU)<br>\n",
    "**device function**<br>\n",
    "&nbsp;&nbsp;&nbsp;a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[https://devblogs.nvidia.com/easy-introduction-cuda-c-and-c/](https://devblogs.nvidia.com/easy-introduction-cuda-c-and-c/)\n",
    "\n",
    "- The CUDA programming model is a heterogeneous model in which both the CPU and GPU are used. \n",
    "\n",
    "- In CUDA, the host refers to the CPU and its memory, while the device refers to the GPU and its memory. \n",
    "\n",
    "- Code run on the host can manage memory on both the host and device, and also launches kernels which are functions executed on the device. These kernels are executed by many GPU threads in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Programming\n",
    "Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model.\n",
    "\n",
    "One feature that significantly simplifies writing GPU kernels is that Numba makes it appear that **the kernel has direct access to NumPy arrays**. NumPy arrays that are supplied as arguments to **the kernel are transferred between the CPU and the GPU automatically** (although this can also be an issue).\n",
    "\n",
    "However, Numba does not implement the full CUDA API, so some features are not available. \n",
    "\n",
    "CUDA support in Numba is being actively developed, so eventually most of the features should be available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Management\n",
    "It is possible to obtain a list of all the GPUs in the system \n",
    "using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the GPUs in the system\n",
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a CUDA-enabled GPU on your system, you will receive one of the following errors:\n",
    "```\n",
    "numba.cuda.cudadrv.error.CudaDriverError: CUDA initialized before forking\n",
    "```\n",
    "```\n",
    "CudaSupportError: Error at driver init: \n",
    "[3] Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED:\n",
    "```\n",
    "```\n",
    "numba.cuda.cudadrv.error.CudaDriverError: Error at driver init:\n",
    "CUDA disabled by user:\n",
    "```\n",
    "If you have a CUDA-enabled GPU you will get the message:\n",
    "```\n",
    "<Managed Device 0>\n",
    "```\n",
    "If your machine has multiple GPUs, you might want to select which one to use. By default the CUDA driver selects the fastest GPU as the device 0, which is the default device used by Numba.\n",
    "```python\n",
    "numba.cuda.select_device( device_id )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CUDA simulator\n",
    "If you don’t have a CUDA-enabled GPU then you will need to use the CUDA simulator. The simulator is enabled by setting the environment variable NUMBA_ENABLE_CUDASIM to 1 in your system (shell command line).\n",
    "```shell\n",
    "export NUMBA_ENABLE_CUDASIM=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the GPUs in the system\n",
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing CUDA kernels\n",
    "- CUDA has an execution model **unlike the traditional sequential model** used for programming CPUs.  \n",
    "\n",
    "\n",
    "- In CUDA, the code you write will be **executed by multiple threads at once** (often hundreds or thousands).     \n",
    "\n",
    "- Your solution will be modeled by defining a **thread hierarchy of grid, blocks, and threads**.      \n",
    "\n",
    "\n",
    "See [https://nyu-cds.github.io/python-gpu/02-cuda/](https://nyu-cds.github.io/python-gpu/02-cuda/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel declaration\n",
    "A kernel function is a GPU function that is meant to be called from CPU code. It has two fundamental characteristics:\n",
    "\n",
    "- kernels cannot explicitly return a value; **all result data must be written to an array passed to the function** (if computing a scalar, you have to pass a one-element array);\n",
    "- kernels explicitly declare their **thread hierarchy** when called: i.e. **the number of blocks per grid** and **the number of threads per block** (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Numba also exposes three kinds of GPU memory:  \n",
    "- global device memory  \n",
    "- shared memory  \n",
    "- local memory   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://docs.nvidia.com/cuda/parallel-thread-execution/graphics/memory-hierarchy.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that you carefully consider how to use and access memory in order to minimize bandwidth requirements and contention.\n",
    "\n",
    "NVIDIA suggests the following recommendations to achieve the best performance:\n",
    "- Find ways to parallelize sequential code\n",
    "- Minimize data transfers between the host and the device\n",
    "- Adjust kernel launch configuration to maximize device utilization\n",
    "- Ensure global memory accesses are coalesced (neighboring threads access contiguous memory locations)\n",
    "- Minimize redundant accesses to global memory whenever possible\n",
    "- Avoid different execution paths within the same warp (warp is a number of threads that can be executed concurrently)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    # code here\n",
    "    io_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel invocation\n",
    "A kernel is typically launched in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the data array here is a simple example and \n",
    "# the data is usually initialized in some other way\n",
    "data = np.ones(256)\n",
    "\n",
    "# Set the number of threads in a block\n",
    "threadsperblock = 32 \n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# or calculate this way:\n",
    "blockspergrid = int(np.ceil(data.size / threadsperblock))\n",
    "# here it is 256 / 32 = 8\n",
    "\n",
    "# Now start the kernel\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "\n",
    "# Print the result\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the block size\n",
    "The two-level thread hierarchy is important for the following reasons:\n",
    "\n",
    "- On the software side, the block size determines how many threads share a given area of shared memory.\n",
    "- On the hardware side, the block size must be large enough for full occupation of execution units; recommendations can be found in the CUDA C Programming Guide. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation\n",
    "\n",
    "The block size you choose depends on a range of factors, including:\n",
    "\n",
    "- The size of the data array\n",
    "- The size of the shared memory per block (e.g. 64KB)\n",
    "- The maximum number of threads per block supported by the hardware (e.g. 512 or 1024)\n",
    "- The maximum number of threads per multiprocessor (MP) (e.g. 2048)\n",
    "- The maximum number of blocks per MP (e.g. 32)\n",
    "- The number of threads that can be executed concurrently (a “warp” i.e. 32)\n",
    "\n",
    "The execution of threads in a warp has a big effect on the computational throughput. \n",
    "\n",
    "- If all threads in a warp are **executing the same instruction** then they **can all be executed in parallel**. \n",
    "\n",
    "- But if one or more threads is **executing a different instruction**, the warp has to be **split into groups** of threads, and these groups execute serially.\n",
    "\n",
    "**Rules of thumb for threads per block:**\n",
    "\n",
    "- Should be a round multiple of the warp size (32)\n",
    "- A good place to start is 128-512 but benchmarking is required to determine the optimal value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread positioning\n",
    "\n",
    "- When running a kernel, the kernel function’s code is executed by every thread once. Therefore **the kernel in each thread needs know which array element(s) it is responsible for**; see output[threadID] below.  \n",
    "\n",
    "\n",
    "- More complex algorithms may define more complex responsibilities, but the underlying principle is the same.  \n",
    "\n",
    "\n",
    "- To help deal with multi-dimensional arrays, CUDA allows you to specify **multi-dimensional blocks and grids**.\n",
    "Blockspergrid and threadsperblock can be 1-dim, 2-dim, or 3-dim (tuples of one, two or three integers). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "---\n",
    "\n",
    "<img src=\"02-threadmapping.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    \n",
    "    # Block id in a 1D grid\n",
    "    bx = cuda.blockIdx.x\n",
    "    \n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    # Compute flattened index inside the array\n",
    "    pos = tx + bx * bw\n",
    "    \n",
    "    if pos < io_array.size: # Check array boundaries\n",
    "        io_array[pos] = 2 * pos # do some computation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "<img src=\"concepts.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.zeros(256)\n",
    "\n",
    "threadsperblock = 32 \n",
    "blockspergrid = int(np.ceil(data.size / threadsperblock)) # here it is 8\n",
    "\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute positions\n",
    "Simple algorithms will tend to always use thread indices in the same way as shown in the example above. Numba provides additional facilities to automate such calculations:\n",
    "\n",
    "**numba.cuda.grid(ndim)**:  Return the absolute position of the current thread in the entire grid of blocks. _ndim_ should correspond to the number of dimensions declared when instantiating the kernel. If ndim is 1, a single integer is returned. If ndim is 2 or 3, a tuple of the given number of integers is returned.\n",
    "\n",
    "**numba.cuda.gridsize(ndim)**: Return the absolute size (or shape) in threads of the entire grid of blocks. _ndim_ has the same meaning as in grid() above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def my_kernel2(io_array):\n",
    "    \n",
    "    pos = cuda.grid(1)\n",
    "    \n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] = 2 * pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.zeros(256)\n",
    "\n",
    "threadsperblock = 32\n",
    "blockspergrid = int(np.ceil(data.size / threadsperblock))\n",
    "\n",
    "my_kernel2[blockspergrid, threadsperblock](data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Real Example: Matrix Multiplication\n",
    "The following code sample is a straightforward implementation of matrix multiplication for matrices where each thread reads one row of A and one column of B and computes the corresponding element of C. For input arrays where A.shape == (m, n) and B.shape == (n, p) then the result shape will be C.shape = (m, p).\n",
    "<img src=\"05-matmul.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    each thread computes one element of C\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    \n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.0\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Programming Model Basics\n",
    "\n",
    "[https://devblogs.nvidia.com/easy-introduction-cuda-c-and-c/](https://devblogs.nvidia.com/easy-introduction-cuda-c-and-c/)\n",
    "\n",
    "\n",
    "Given the heterogeneous nature of the CUDA programming model, a typical sequence of operations for a CUDA C program is:\n",
    "\n",
    "- Declare and allocate host and device memory.  \n",
    "- Initialize host data.  \n",
    "- Transfer data from the host to the device.  \n",
    "- Execute one or more kernels.  \n",
    "- Transfer results from the device to the host.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host code\n",
    "from numba import cuda\n",
    "import numpy \n",
    "\n",
    "# Initialize the data arrays\n",
    "A = numpy.full((24, 16), 3, numpy.float) # matrix containing all 3's\n",
    "B = numpy.full((16, 24), 4, numpy.float) # matrix containing all 4's\n",
    "\n",
    "# Copy the arrays to the device (GPU)\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device (GPU) for the result\n",
    "# C_global_mem = cuda.device_array((24, 24), dtype=numpy.float)\n",
    "C_global_mem = cuda.device_array((24, 24))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(numpy.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(numpy.ceil(B.shape[1] / threadsperblock[1]))\n",
    "# two dimenesional block-per-grid\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "print(\"blockspergrid: \", blockspergrid)\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host\n",
    "C = C_global_mem.copy_to_host()\n",
    "\n",
    "print(\"C=\\n\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with this code is that each thread is reading from the global memory containing the copies of A and B. In fact, the A global memory is read B.shape[1] times and the B global memory is read A.shape[0] times. Since global memory is fairly slow, this results in an inefficient use of the GPU.\n",
    "\n",
    "Solution: use **shared memory**! (more in your lab)\n",
    "* copy small blocks of the matrix into shared memory\n",
    "* synchronize threads to wait until all threads have access to this new shared memory\n",
    "* compute partial sums only using the small block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://nyu-cds.github.io/python-numba/fig/05-matmulshared.png' style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp\n",
    "\n",
    "# The data array\n",
    "A = numpy.full((TPB*2, TPB*3), 3, numpy.float) # [32 x 48] matrix containing all 3's\n",
    "B = numpy.full((TPB*3, TPB*1), 4, numpy.float) # [48 x 16] matrix containing all 4's\n",
    "\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "C_global_mem = cuda.device_array((TPB*2, TPB*1)) # [32 x 16] matrix result\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[1]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[0]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "res = C_global_mem.copy_to_host()\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
